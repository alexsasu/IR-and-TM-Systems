{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install chromadb\n",
    "!pip install --upgrade --quiet  docx2txt\n",
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install sentence-transformers\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-25T14:52:34.673452Z",
     "iopub.status.busy": "2025-01-25T14:52:34.673200Z",
     "iopub.status.idle": "2025-01-25T14:52:54.960330Z",
     "shell.execute_reply": "2025-01-25T14:52:54.959684Z",
     "shell.execute_reply.started": "2025-01-25T14:52:34.673431Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader, Docx2txtLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T15:53:03.423323Z",
     "iopub.status.busy": "2025-01-24T15:53:03.422811Z",
     "iopub.status.idle": "2025-01-24T15:53:03.426885Z",
     "shell.execute_reply": "2025-01-24T15:53:03.426139Z",
     "shell.execute_reply.started": "2025-01-24T15:53:03.423298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "folder_path = \"/kaggle/input/irtm-project-2-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T15:53:03.429922Z",
     "iopub.status.busy": "2025-01-24T15:53:03.429717Z",
     "iopub.status.idle": "2025-01-24T15:53:03.594182Z",
     "shell.execute_reply": "2025-01-24T15:53:03.593297Z",
     "shell.execute_reply.started": "2025-01-24T15:53:03.429904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path)\n",
    "            txt = loader.load()\n",
    "            documents.extend(txt)\n",
    "        elif filename.endswith(\".doc\") or filename.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            doc = loader.load()\n",
    "            documents.extend(doc)\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pdf = loader.load()\n",
    "            documents.extend(pdf)\n",
    "\n",
    "    # This is where data chunking is realized\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=40)\n",
    "    documents_split = splitter.split_documents(documents)\n",
    "    \n",
    "    return documents_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T21:11:02.075867Z",
     "iopub.status.busy": "2025-01-24T21:11:02.075526Z",
     "iopub.status.idle": "2025-01-24T21:11:02.080598Z",
     "shell.execute_reply": "2025-01-24T21:11:02.079699Z",
     "shell.execute_reply.started": "2025-01-24T21:11:02.075839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_documents(documents):\n",
    "    # We replace unusual characters with their normal counterpart\n",
    "    for i in range(len(documents)):\n",
    "        documents[i].page_content = documents[i].page_content.replace('“', '\"').replace('”', '\"')\n",
    "        documents[i].page_content = documents[i].page_content.replace('ş', 'ș').replace(\"Ş\", \"Ș\").replace('ţ', 'ț').replace(\"Ţ\", \"Ț\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T15:53:03.611518Z",
     "iopub.status.busy": "2025-01-24T15:53:03.611237Z",
     "iopub.status.idle": "2025-01-24T15:53:19.341126Z",
     "shell.execute_reply": "2025-01-24T15:53:19.340434Z",
     "shell.execute_reply.started": "2025-01-24T15:53:03.611485Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "documents = load_documents(folder_path)\n",
    "documents = preprocess_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T16:19:59.305342Z",
     "iopub.status.busy": "2025-01-24T16:19:59.304924Z",
     "iopub.status.idle": "2025-01-24T16:19:59.309552Z",
     "shell.execute_reply": "2025-01-24T16:19:59.308664Z",
     "shell.execute_reply.started": "2025-01-24T16:19:59.305285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def initialize_vector_db(docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    chroma_db = Chroma.from_documents(persist_directory=\"db\", documents=docs, embedding=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    return chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = initialize_vector_db(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hf_pipeline = pipeline(\"text-generation\", model=\"/kaggle/input/openllm-ro-q4b/transformers/7b-instruct-q4b/1/RoLlama2-7b-Instruct-v1-q4b\", kwargs=['_load_in_4bit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T00:22:44.611278Z",
     "iopub.status.busy": "2025-01-25T00:22:44.610981Z",
     "iopub.status.idle": "2025-01-25T00:22:44.615975Z",
     "shell.execute_reply": "2025-01-25T00:22:44.615026Z",
     "shell.execute_reply.started": "2025-01-25T00:22:44.611253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_query_context(query):\n",
    "    # The IR component\n",
    "    documents = chroma_db.similarity_search_with_score(query, k=15)\n",
    "\n",
    "    print(f\"{len(documents)} context documents\")\n",
    "    for d in documents:\n",
    "        print(f\"------------------ similarity (less is better) {round(d[1], 2)}\")\n",
    "        print(f\"document: {d[0].metadata['source'].split('/')[4]} || context: {d[0].page_content}\")\n",
    "    print(\"------------------\\n\")\n",
    "\n",
    "    context = \" \".join([f\"document: {doc[0].metadata['source'].split('/')[4]} || context: {doc[0].page_content}\" for doc in documents])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T00:54:19.734988Z",
     "iopub.status.busy": "2025-01-25T00:54:19.734700Z",
     "iopub.status.idle": "2025-01-25T00:54:19.738665Z",
     "shell.execute_reply": "2025-01-25T00:54:19.737813Z",
     "shell.execute_reply.started": "2025-01-25T00:54:19.734963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_prompt_rag(query, context):\n",
    "    prompt = f\"\"\"\n",
    "    ---Instrucțiuni---:\n",
    "    Scrie un răspuns prietenos și ușor de înțeles pentru întrebarea din cadrul câmpului ---Întrebare---:\n",
    "    Genereaza doar informațiile aferente câmpului ---Răspuns---: fără să oferi context sau alte informații suplimentare\n",
    "    Mai jos sunt trei exemple, ---Exemplu 1---:, ---Exemplu 2---:, și ---Exemplu 3---:, care arată cum va trebui sa răspunzi întrebării din cadrul câmpului ---Întrebare---:\n",
    "    Încearcă să generezi răspunsul pe baza faptelor din câmpul ---Context---:. Dacă în datele pe care ai fost antrenat nu se află informațiile necesare pentru a răspunde la ---Întrebare---: atunci generează mesajul \"Nu pot oferi un răspuns concret.\"\n",
    "    \n",
    "    ---Exemplu 1---:\n",
    "    Întrebare: Ce este schimbarea climatică?\n",
    "    Răspuns: Schimbarea climatică este atunci când vremea planetei noastre se modifică pe o perioadă lungă, de obicei din cauza activităților umane, cum ar fi arderea combustibililor fosili.\n",
    "    \n",
    "    ---Exemplu 2---:\n",
    "    Întrebare: Care este capitala României?\n",
    "    Răspuns: Capitala României este București.\n",
    "    \n",
    "    ---Exemplu 3---:\n",
    "    Întrebare: Cum funcționează energia solară?\n",
    "    Răspuns: Energia solară funcționează prin captarea luminii de la soare folosind panouri solare. Aceste panouri transformă lumina în electricitate pe care o putem folosi acasă.\n",
    "    \n",
    "    ---Context---:\n",
    "    {context}\n",
    "    \n",
    "    ---Întrebare---: {query} Care sunt maximum top 3 documente relevante, cu nume diferite, pentru propoziția anterioară?\n",
    "    ---Răspuns---: \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T00:22:39.084428Z",
     "iopub.status.busy": "2025-01-25T00:22:39.084109Z",
     "iopub.status.idle": "2025-01-25T00:22:39.088338Z",
     "shell.execute_reply": "2025-01-25T00:22:39.087545Z",
     "shell.execute_reply.started": "2025-01-25T00:22:39.084402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_prompt_llm(query):\n",
    "    prompt = f\"\"\"\n",
    "    ---Instrucțiuni---:\n",
    "    Scrie un răspuns prietenos și ușor de înțeles pentru întrebarea din cadrul câmpului ---Întrebare---:\n",
    "    Genereaza doar informațiile aferente câmpului ---Răspuns---: fără să oferi context sau alte informații suplimentare\n",
    "    Mai jos sunt trei exemple, ---Exemplu 1---:, ---Exemplu 2---:, și ---Exemplu 3---:, care arată cum va trebui sa răspunzi întrebării din cadrul câmpului ---Întrebare---:\n",
    "    Dacă în datele pe care ai fost antrenat nu se află informațiile necesare pentru a răspunde la ---Întrebare---: atunci generează mesajul \"Nu pot oferi un răspuns concret.\"\n",
    "    \n",
    "    ---Exemplu 1---:\n",
    "    Întrebare: Ce este schimbarea climatică?\n",
    "    Răspuns: Schimbarea climatică este atunci când vremea planetei noastre se modifică pe o perioadă lungă, de obicei din cauza activităților umane, cum ar fi arderea combustibililor fosili.\n",
    "    \n",
    "    ---Exemplu 2---:\n",
    "    Întrebare: Care este capitala României?\n",
    "    Răspuns: Capitala României este București.\n",
    "    \n",
    "    ---Exemplu 3---:\n",
    "    Întrebare: Cum funcționează energia solară?\n",
    "    Răspuns: Energia solară funcționează prin captarea luminii de la soare folosind panouri solare. Aceste panouri transformă lumina în electricitate pe care o putem folosi acasă.\n",
    "    \n",
    "    ---Întrebare---: {query} Care sunt maximum top 3 documente relevante, cu nume diferite, pentru propoziția anterioară?\n",
    "    ---Răspuns---: \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T00:31:08.933378Z",
     "iopub.status.busy": "2025-01-25T00:31:08.933064Z",
     "iopub.status.idle": "2025-01-25T00:31:08.938428Z",
     "shell.execute_reply": "2025-01-25T00:31:08.937612Z",
     "shell.execute_reply.started": "2025-01-25T00:31:08.933349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_system_response(query, system, temperature=0.1, top_p=0.8, max_new_tokens=512):\n",
    "    context = get_query_context(query)\n",
    "\n",
    "    if system == \"rag\":\n",
    "        prompt = get_prompt_rag(query, context)\n",
    "    else:\n",
    "        prompt = get_prompt_llm(query)\n",
    "    \n",
    "    system_output = hf_pipeline(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    response = system_output[0]['generated_text']\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Cine este Lică Sămădăul?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "\n",
    "system = \"rag\"\n",
    "response = get_system_response(query, system, max_new_tokens=128)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "system = \"llm\"\n",
    "response = get_system_response(query, system, max_new_tokens=128)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR\n",
    "\n",
    "response = get_query_context(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6521335,
     "sourceId": 10564962,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 60219,
     "modelInstanceId": 43578,
     "sourceId": 51817,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
